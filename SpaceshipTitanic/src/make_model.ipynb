{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a1f0be",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5edd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821c556",
   "metadata": {},
   "source": [
    "# Define base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af8a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf12991",
   "metadata": {},
   "source": [
    "# File List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03bd6b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data List\n",
      "['description', 'sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "print(\"Data List\")\n",
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e0a7f",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a5e759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:(8693, 14)\n",
      "test shape:(4277, 13)\n",
      "sample_submission shape:(4277, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "print(f\"train shape:{train.shape}\")\n",
    "print(f\"test shape:{test.shape}\")\n",
    "print(f\"sample_submission shape:{sample_submission.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c8bc6",
   "metadata": {},
   "source": [
    "# Split with features and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f73e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ans = train[\"Transported\"] * 1.0\n",
    "train = train.drop(\"Transported\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f6733",
   "metadata": {},
   "source": [
    "# => Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf08543",
   "metadata": {},
   "source": [
    "## Define data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "967c8536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Changed=================\n",
      "column:<PassengerId>  object    -->    object\n",
      "column:<HomePlanet>  object    -->    category\n",
      "column:<CryoSleep>  object    -->    boolean\n",
      "column:<Cabin>  object    -->    category\n",
      "column:<Destination>  object    -->    category\n",
      "column:<Age>  float64    -->    float64\n",
      "column:<VIP>  object    -->    boolean\n",
      "column:<RoomService>  float64    -->    float64\n",
      "column:<FoodCourt>  float64    -->    float64\n",
      "column:<ShoppingMall>  float64    -->    float64\n",
      "column:<Spa>  float64    -->    float64\n",
      "column:<VRDeck>  float64    -->    float64\n",
      "column:<Name>  object    -->    category\n"
     ]
    }
   ],
   "source": [
    "old_dtypes = train.dtypes\n",
    "\n",
    "dtype_dict = {\n",
    "    \"PassengerId\": \"object\",\n",
    "    \"HomePlanet\": \"category\",\n",
    "    \"CryoSleep\": \"boolean\",\n",
    "    \"Cabin\": \"category\",\n",
    "    \"Destination\": \"category\",\n",
    "    \"Age\":\"float\",\n",
    "    \"VIP\": \"boolean\",\n",
    "    \"RoomService\": \"float\",\n",
    "    \"FoodCourt\": \"float\",\n",
    "    \"ShoppingMall\": \"float\",\n",
    "    \"Spa\": \"float\",\n",
    "    \"VRDeck\": \"float\",\n",
    "    \"Name\": \"category\",    \n",
    "}\n",
    "train = train.astype(dtype_dict)\n",
    "new_dtypes = train.dtypes\n",
    "print(\"===============Changed=================\")\n",
    "for _index, _old, _new in zip(old_dtypes.index, old_dtypes, new_dtypes):\n",
    "    print(f\"column:<{_index}>  {_old}    -->    {_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528f916",
   "metadata": {},
   "source": [
    "## Drop unused features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bce421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop([\"Name\", \"PassengerId\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff32b49",
   "metadata": {},
   "source": [
    "## Split merged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "570d337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = train[\"Cabin\"].str.split(\"\\/\", expand=True)\n",
    "sub_df.columns = [\n",
    "    \"Cabin_A\",\n",
    "    \"Cabin_B\",\n",
    "    \"Cabin_C\"\n",
    "]\n",
    "sub_df = sub_df.astype({\n",
    "    \"Cabin_A\": \"category\",\n",
    "    \"Cabin_B\": \"float\",\n",
    "    \"Cabin_C\": \"category\"\n",
    "})\n",
    "train = pd.concat([train, sub_df], axis=1)\n",
    "train = train.drop(\"Cabin\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15947d6",
   "metadata": {},
   "source": [
    "## Null padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c555d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age',\n",
       " 'Cabin_B',\n",
       " 'CryoSleep',\n",
       " 'FoodCourt',\n",
       " 'RoomService',\n",
       " 'ShoppingMall',\n",
       " 'Spa',\n",
       " 'VIP',\n",
       " 'VRDeck'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_features = set(train.isnull().sum()[train.isnull().sum() > 0].index)\n",
    "target_features &= set(train.dtypes[(train.dtypes == \"float\") | (train.dtypes == \"boolean\")].index)\n",
    "target_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d897d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in target_features:\n",
    "    null_colmun_name = column + \"_NULL\"\n",
    "    train[null_colmun_name] = train[column].isna() * 1.0\n",
    "    train[column] = train[column].fillna(0.0)\n",
    "    # for boolean\n",
    "    train[column] = train[column] * 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71442884",
   "metadata": {},
   "source": [
    "## One-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6980c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only categorical features\n",
    "encoders = {}\n",
    "for column in train.columns:\n",
    "    if train[column].dtype != \"category\":\n",
    "        continue\n",
    "    arr_data = np.array(train[column].values).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder().fit(arr_data)\n",
    "    sub_df = pd.DataFrame(\n",
    "        encoder.transform(arr_data).toarray(),\n",
    "        columns = [f\"{column}_{_category}\" for _category in encoder.categories_[0]]\n",
    "    )\n",
    "    train = train.drop(column, axis=1)\n",
    "    train = pd.concat([train, sub_df], axis=1)\n",
    "    encoders[column] = encoder\n",
    "with open(\"./model/encoders.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoders, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a33f90",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37957fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(srs):\n",
    "    return (srs - srs.min()) / (srs.max() - srs.min())\n",
    "\n",
    "for _column in train.columns:\n",
    "    train[_column] = norm(train[_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c977d9",
   "metadata": {},
   "source": [
    "## Stack Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d5013dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Input, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e444db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 38)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               19968     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 512)              2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 419,329\n",
      "Trainable params: 416,769\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(train.shape[1]))\n",
    "layer = Dense(512, activation=\"relu\")(inputs)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "layer = Dense(512, activation=\"relu\")(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "\n",
    "layer = Dense(256, activation=\"relu\")(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "\n",
    "layer = Dense(1, activation=\"sigmoid\")(layer)\n",
    "dl_model = Model(inputs=inputs, outputs=layer)\n",
    "dl_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cd613",
   "metadata": {},
   "source": [
    "## Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "678bcc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "        loss_weights=None,\n",
    "        sample_weight_mode=None,\n",
    "        weighted_metrics=None,\n",
    "        target_tensors=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c661b",
   "metadata": {},
   "source": [
    "## Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccd90e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=3\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=\"./model/model.h5\",\n",
    "            vervose=1,\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc5f9c0",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3fcb564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000253FF938828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000253FF938828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "52/55 [===========================>..] - ETA: 0s - loss: 0.6297 - accuracy: 0.6980WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000253945A8A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000253945A8A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "55/55 [==============================] - 3s 19ms/step - loss: 0.6268 - accuracy: 0.6993 - val_loss: 0.6536 - val_accuracy: 0.5423\n",
      "Epoch 2/100\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.5301 - accuracy: 0.7295 - val_loss: 0.6200 - val_accuracy: 0.6584\n",
      "Epoch 3/100\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4793 - accuracy: 0.7613 - val_loss: 0.6000 - val_accuracy: 0.6406\n",
      "Epoch 4/100\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4604 - accuracy: 0.7659 - val_loss: 0.5478 - val_accuracy: 0.7062\n",
      "Epoch 5/100\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.4422 - accuracy: 0.7785 - val_loss: 0.5279 - val_accuracy: 0.7556\n",
      "Epoch 6/100\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4204 - accuracy: 0.7908 - val_loss: 0.5050 - val_accuracy: 0.7223\n",
      "Epoch 7/100\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4168 - accuracy: 0.7939 - val_loss: 0.4876 - val_accuracy: 0.7487\n",
      "Epoch 8/100\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4098 - accuracy: 0.8011 - val_loss: 0.5148 - val_accuracy: 0.7182\n",
      "Epoch 9/100\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4049 - accuracy: 0.7988 - val_loss: 0.4547 - val_accuracy: 0.7746\n",
      "Epoch 10/100\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3970 - accuracy: 0.8007 - val_loss: 0.5009 - val_accuracy: 0.7539\n",
      "Epoch 11/100\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3875 - accuracy: 0.8069 - val_loss: 0.5416 - val_accuracy: 0.7504\n",
      "Epoch 12/100\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3869 - accuracy: 0.8076 - val_loss: 0.6118 - val_accuracy: 0.7349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x253940be388>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_model.fit(\n",
    "    train.astype(\"float\").values,\n",
    "    train_ans.astype(\"float\").values,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    class_weight=None,\n",
    "    sample_weight=None,\n",
    "    initial_epoch=0,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a0274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
